{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn SQL Basics for Data Science Specialization\n",
    "## Lesson 2: Data Wrangling数据整理, Analysis and AB Testing with SQL\n",
    "\n",
    "### w1: what we can trust in dataset  \n",
    "### w2: build trustworthy tables  \n",
    "the way data is stored, how it's moed around, how it ultimately ends up in the tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 1:Data of Unknown Quality\n",
    "\n",
    "#### Objectives:\n",
    "1. main categories of data types\n",
    "2. manipulate unfiltered data into a table where conducting data analysis\n",
    "3. why a data warehouse is separate from a production database?\n",
    "4. create your own trustworthy tables\n",
    "\n",
    "## Required knowledge: \n",
    "SELECT  \n",
    "WHERE  \n",
    "GROUP and ORDER BY  \n",
    "Inner and Left joins  \n",
    "logical operators  \n",
    "CAST( ):convert datatype\n",
    "```sql\n",
    "SELECT CAST(25.65 AS int);\n",
    ">>>25\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "- Aggregation functions(MAX,COUNT,SUM,etc): use with `GROUP BY` \n",
    "- CASE,WHEN,IF,COALESCE  \n",
    "- Subqueries  \n",
    "- Windowing functions  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### 1. Pivot data  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font color=hotpink> 2. star schema vs Snowflake Schema </font>\n",
    "    \n",
    "### <font color=deeppink> 1. star schema </font> (**架构** *模式计划图解概要*):  \n",
    "1. the simplest style of **data mart schema数据集市模型** \n",
    "2. the most widely used approach to develop开发 **data warehouses数据仓库 and dimensional data marts维度数据集市**.\n",
    "3. https://developer.aliyun.com/article/576960\n",
    "    \n",
    "star schema主要思想：将关心的数据和用于描述数据的属性 分开  \n",
    "实际数据存Fact table  \n",
    "从不同角度来描述数据的属性放到不同的dimension table  \n",
    "\n",
    "fact table和dimension table间`主-外键`关联，各dimension table之间无关。\n",
    "（fact table的主键很有可能是所有dimension table的外键组合起来的一个组合主键）\n",
    " \n",
    "star schema之所以叫star schema，就是由于上面这个图形的形状来的，fact table处于中间的位置，dimension table围成一圈，每个dimension table和fact table关联。\n",
    "![c](https://live.staticflickr.com/65535/51804967616_7ff9abf0ef_c.jpg)\n",
    "\n",
    "\n",
    "fact table除了区分每条记录的主键，连接每个dimension table的外键外，  \n",
    "就只有我们关心的数字型数据，所以fact table中的每条记录，有个专门的术语称之为<mark>度量(measurement)</mark>，  \n",
    "\n",
    "用数据仓库做统计分析时，这些数据就是统计分析的一个个基本单位，即度量值。  \n",
    "\n",
    "\n",
    "#### <font color=violetred>**star transformation星型转换**</font>:  \n",
    "强大的优化技术，通过对原SQL语句的隐式改写来实现，能很大程度减少I/O. 终端用户无需知道星型转换的细节\n",
    "数据库优化器会在合适时机进行星型转换。\n",
    "\n",
    "要获得星型转换的最大性能，需遵以下3个条件： www.2cto.com  \n",
    ">1. 事实表上的维度列有外键  \n",
    ">2. 事实表的每个外键上有BITMAP索引  \n",
    ">3. star_transformation_enabled=true. 系统默认false. 它有三个取值{TRUE, FALSE, TEMP_DISABLE}.TEMP_DISABLE:不允许用临时表来存放第一次扫描的结果集  \n",
    " \n",
    "星型查询中，维度表会被扫描两次，如维度表大，性能会差，所以需一个临时表来存放第一次扫描的维度表集合  \n",
    "如满足这三个条件,则查询会使用star transformation,而这是提高 基于事实表的查询效率 的主要技术  \n",
    "\n",
    "数据库进行星型查询时，使用两个基本阶段：\n",
    "第一个阶段:从事实表(=结果集)里获取所有必要的记录行。这是通过<mark>bitmap索引</mark>来检索数据，高效 \n",
    " \n",
    "第二个阶段:将该结果集与维度表进行关联。这叫做<mark>semi-join<mark>(也就是exists和in写法)。\n",
    "(注：只有oracle企业版才有bitmap索引 标准版不支持bitmap索引和星型转换)\n",
    "\n",
    "    \n",
    "    \n",
    "### <font color=deeppink> 2.Snowflake Schema </font>\n",
    "\n",
    "    \n",
    "---\n",
    "### <font color=deeppink> 3. comparison </font>   \n",
    " https://blog.csdn.net/u012988208/article/details/46817407\n",
    "       \n",
    "<font color=violetred>    </font>       \n",
    "        \n",
    "#### <font color=violetred>  3.1 Star Schema </font>\n",
    "\n",
    "<mark>事实表</mark>被若干<mark>维度表</mark>包围。每个维度代表一张表，有<mark>主键</mark>关联事实表中的<mark>外键</mark>\n",
    "\n",
    "- 所有事实必须同一<mark>粒度</mark>\n",
    "\n",
    "- 不同维度间没有关联\n",
    "\n",
    "![c](https://live.staticflickr.com/65535/51805454445_ec1607c5be_n.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### <font color=violetred>  3.2 Snowflake Schema </font>\n",
    "\n",
    "基于星型拓展，每一维度可再扩散出更多维度，根据维度层级拆分成<mark>颗粒度不同</mark>的多张表  \n",
    "\n",
    "    \n",
    "\n",
    "对dimension table规范化后的模型，每个dimension table规范化可能得到许多的小表，雪花模型更复杂，查询的时候也需关联更多的表。\n",
    "oracle在文档中说，除非你有非常特别的原因，推荐采用star schema来进行数据仓库的架构设计。\n",
    "\n",
    "\n",
    "\n",
    "- 优点:减少维度表的数据量，`join`查询时提升速度\n",
    "\n",
    "- 缺点:额外维护维度表的数量\n",
    "![c](https://live.staticflickr.com/65535/51805085634_f1a50f5ff2.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. data wharehousing: how different types of data are classified + stored  \n",
    "\n",
    "    \n",
    "    \n",
    "## [video:flexible data formates]\n",
    "   \n",
    "   学了怎么用`CASE`把同一column中不同type的data分离变成新的两个column  \n",
    "   parameter_name: item_id, referrer\n",
    "    \n",
    "    \n",
    "## [video: Identifying Unreliable Data + Nulls]\n",
    "Goal:   \n",
    "    1. segment and visualize data to check for completeness  \n",
    "    2.  might be able to identify reasons why data might be missing  \n",
    "\n",
    "    \n",
    "    \n",
    "Steps:1. look at the name of the table:\n",
    "      1.1 table name has 'temp' or a 'date', it was created for a one-off analysis and it may no longer reflect the current state of the world. \n",
    "    1.2 look at the <mark>schema</mark> of the table is in. =look at the words before the dot. \n",
    "            (<mark>warehouse</mark>.users_20180105)\n",
    "            You might already know where to find some official tables for this class, a lot of it is in the <mark>DSV1069</mark> schema. But there are lots of other schemas in the mode public warehouse. \n",
    "      1.3 Another hint is how long the name is. If very long, table is for a specific purpose\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    " each event was implemented by hand. For each event type, a developer was assigned the task of specifically recording that event. Just because you don't see events for some time period, that can't be taken to mean that the action wasn't done, it only means it wasn't recorded. \n",
    "    \n",
    "\n",
    "    \n",
    "`COALESCE` replace a value in a case it's null\n",
    "    \n",
    "\n",
    "1. Can you trust the your data?\n",
    "    - missing days of data\n",
    "    - duplicate event rows\n",
    "    \n",
    "    \n",
    "    \n",
    "2. In this exercise, the question is coming from the CEO, who wants to check this number every day in the morning, and we'll be presenting these numbers at board meetings. What if instead the person asking was trying to decide if we needed to upgrade to a bigger production database this month, or if we could just do it next year? They might need an order of magnitude answer, with a little bit of a forecast and the answer might be yes or no, rather than the number they initially asked for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 2:Creating Clean Datasets\n",
    "\n",
    "\n",
    "### Terms:  \n",
    "- Dependency, pipline, ETL(extract transform load)system\n",
    "\n",
    "- stale when the data in atable doesn't reflect the most up-to-date info\n",
    "\n",
    "- Backfill:\n",
    "\n",
    "- Data engineering: make data available, managing software that schedules the pipline))\n",
    "\n",
    "- pipline: several table vreation/update task on a range of dates in the past\n",
    "\n",
    "- job: the task given to database to perform ETL\n",
    "\n",
    "\n",
    "## I. create table based on event stream  \n",
    "\n",
    "`DROP TABLE`: delete table\n",
    "\n",
    "```sql\n",
    "CREAT TABLE 'view_item-events'\n",
    "(\n",
    "    event_id VARCHAR(32) NOT NULL PRIMARY KEY,\n",
    "    event_time VARCHAR(26),\n",
    "    user_id INT(10),\n",
    "    platform VARCHAR(10),\n",
    "    item_id INT(10),\n",
    "    referrer VARCHAR(17)\n",
    "    \n",
    ");\n",
    " \n",
    "```\n",
    "\n",
    "```sql\n",
    "CREAT TABLE IF NOT EXISTS 'view_item-events'\n",
    "(\n",
    "    event_id VARCHAR(32) NOT NULL PRIMARY KEY,\n",
    "    event_time VARCHAR(26),\n",
    "    user_id INT(10),\n",
    "    platform VARCHAR(10),\n",
    "    item_id INT(10),\n",
    "    referrer VARCHAR(17)\n",
    "    \n",
    ");\n",
    " \n",
    "```\n",
    "\n",
    "\n",
    "`INSERT INTO`take the data that you query and insert into the table\n",
    "\n",
    "problem: add duplicate data cuz you do it over and over again  \n",
    "solve: use `REPLACE INTO`(MySQL specific syntax, `INSERT OVERWRITE`) replace everything in the table\n",
    "\n",
    "\"Realistically, the process of putting the data into the dimension tables and the production device and copying them over to an analytics database is non trivial amount of work, as is **setting up the events pipeline**.\"\n",
    "\n",
    "\n",
    "hierarchy of data  \n",
    "aggregating and labeling data: to segment ->snapshot table\n",
    "use `windowing functions`  \n",
    "\n",
    "dependencies: up stream, downstream\n",
    "\n",
    "backfilling: schedule a query each day\n",
    "\n",
    "\n",
    "## II. create user snapshot stable\n",
    "\n",
    "#### 1. <font color=hotpink>scheduling query</font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Insert column\n",
    "\n",
    "#### 2. <font color=hotpink>variable tags</font>: liquid language, written in Ruby, created by Shopify\n",
    "```sql\n",
    "--liquid tags\n",
    "{% assign ds='2018-01-01'%}\n",
    "SELECT\n",
    "    id,\n",
    "    '{{ds}}' AS variable_column  \n",
    "FROM users \n",
    "WHERE\n",
    "    created_at <='{{ds}}'\n",
    "```\n",
    "|id|variable_column|\n",
    "|---|---|\n",
    "|1|2018-01-01|\n",
    "|2|2018-01-01|\n",
    "|3|2018-01-01|\n",
    "\n",
    "\n",
    "## Task: insert user info table\n",
    "need to insert: user id, whether or not user was created today, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON:\n",
    "JavaScript Object Notation\n",
    "\n",
    "\n",
    "`Hive蜂巢分区`:data warehousing infrastructure. distributed data system  \n",
    "`Partitions分区` are way of subdividing细分 a table into a piece that can be stored or queried individually. make large query more efficient  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hadoop`:  \n",
    "Apache基金会所开发的**分布式系统基础架构**,户可以在**不了解分布式底层细节**的情况下，开发分布式程序。充分利用集群的威力进行**高速运算和存储**。  \n",
    "Hadoop实现了一个分布式文件系统（ Distributed File System），其中一个组件是HDFS（**Hadoop Distributed File System**）。HDFS有**高容错性**的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（**large data set**）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最**核心**的设计就是：HDFS和MapReduce。**HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算** \n",
    "\n",
    "\n",
    "1. d to store large dataset on multiple servers(-cluster=nodes)\n",
    "to retrieve data, need to make nodes communicate to each other, Hadoop is a way to do that.  \n",
    "\n",
    "2. \n",
    "Hive(HQL: Hive SQL) is a tool that translates sql into something that can be run on nodes that running Hadoop.  \n",
    " \n",
    "3. \n",
    "Partitions: a feature of Hive. partitions doesn't change the functional correctness of a query when you're retrieving data, but it changes the way you create and insert data into the table. \n",
    "\n",
    "what to partition:\n",
    "1. date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 3:SQL Problem Solving\n",
    "\n",
    "|    | Video                                                       | Practice quiz            | Discuss exercise    |\n",
    "|----|-------------------------------------------------------------|--------------------------|---------------------|\n",
    "| 1  | map out joins                                               | reorder & connect tables |                     |\n",
    "| 2  | test queries & final queries                                |                          |                     |\n",
    "| 3  | create aggregate table w/ rolling date period               |                          | create rollup table |\n",
    "| 4  | rolling orders(solution)                                    |                          |                     |\n",
    "| 5  | find each user's most recent viewed page for Email Campaign |                          |                     |\n",
    "| 6  | Review Windowing functions                                  |                          | promo email         |\n",
    "| 7  | promo email(solution)                                       |                          | email campaign      |\n",
    "| 8  | product analysis                                            |                          | product analysis    |\n",
    "| 9  | product analysis(solution)                                  |                          |                     |\n",
    "| 10 | coding w/ style                                             |                          |                     |\n",
    "\n",
    "\n",
    "\n",
    "# 1. map out joins\n",
    "\n",
    "exploratory analysis, plan how to query, develop a frame of query\n",
    "\n",
    "ex)company is considering building a better widget recommendation system, is this a good idea?\n",
    "\n",
    "some questions sql can answer:\n",
    "1. how many widget related items do we sell? \n",
    "2. What percent of users have viewed a widget page? \n",
    "3. we could also be building a new contraption recommender, would that be a better project? \n",
    "4. What percent of users have used items from each product category?  \n",
    "    4.1 columns needed  \n",
    "    4.2 tables needed  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|   | **columns needed**                              | **table needed** |      |   |   |\n",
    "|---|-------------------------------------------------|------------------|------|---|---|\n",
    "|   | category                                        | viewed_items     | join |   |   |\n",
    "|   | count of users                                  |                  |      |   |   |\n",
    "|   | total user number(in order to calculate user %) | users table      |      |   |   |\n",
    "|   |                                                 |                  |      |   |   |\n",
    "|   | user ids                                        |                  |      |   |   |\n",
    "|   | item ids                                        | items            | join |   |   |\n",
    "\n",
    "\n",
    "\n",
    "    4.3 subtable needed (build the subqueries)\n",
    "        -'viewed-item events table' is already built in module 1, we'll put it in its own table in module 2\n",
    "        -total users:subquery, but don't need it ?\n",
    "        -\n",
    "\n",
    "    4.4 test joins\n",
    "   \n",
    "```sql\n",
    "SELECT *\n",
    "FROM viewed_items\n",
    "JOIN items\n",
    "ON items_id = viewed_items.item_id\n",
    "```\n",
    "\n",
    "    4.4 add columns: count distinct, so also need to add group by\n",
    "```sql\n",
    "SELECT items.category, COUNT(DISTINCT user_id)\n",
    "FROM viewed_items\n",
    "JOIN items\n",
    "ON items.id = viewed_items.item_id\n",
    "GROUP BY items.category\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 4:Case Study: AB Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
